{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'../..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "hyspeclab_dir = os.path.join(project_dir, 'HySpecLab')\n",
    "if hyspeclab_dir not in sys.path:\n",
    "    sys.path.append(hyspeclab_dir)\n",
    "\n",
    "ipdl_dir = os.path.join(project_dir, 'modules/IPDL')\n",
    "if ipdl_dir not in sys.path:\n",
    "    sys.path.append(ipdl_dir)    \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.dataset import DermaDataset\n",
    "\n",
    "train_dir = ['train', 'validation']\n",
    "dataset_dir = list(map(lambda x: os.path.join(config.DERMA_DATASET_DIR, x), train_dir))\n",
    "\n",
    "dataset = DermaDataset(dataset_dir)\n",
    "x, y = dataset.get(dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Z-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized = (x.values - x.mean().values) / x.std().values\n",
    "x_normalized = pd.DataFrame(x_normalized, columns=x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction\n",
    "\n",
    "Due to a computational limitation, it is now only possible to apply the selection of features by Bayesian optimization up to 64 features. This step is used in order to reduce the number of features based on Tree-based feature importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HySpecLab.feature_selection import InterbandRedundacyMutualInformationSelector\n",
    "\n",
    "selector = InterbandRedundacyMutualInformationSelector(threshold=10, undersampling=256, gamma=.5)\n",
    "selector.fit(x_normalized.values, y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features Selected: {}'.format(np.where(selector.mask_)[0]))\n",
    "X_new = selector.transform(x_normalized.values)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "# Randomly selecting a subset of data for the targeted classes:\n",
    "# rus = RandomUnderSampler(random_state=123)\n",
    "# x, y = rus.fit_resample(x, y)\n",
    "\n",
    "# # Let positive samples be the samples belonging to the targeted class to be under-sampled. \n",
    "# # Negative sample refers to the samples from the minority class.\n",
    "# # Select the positive samples for which the average distance to the N closest samples of the negative class is the smallest\n",
    "sampling_strategy = {0: 512, 1: 512}\n",
    "nm = NearMiss(sampling_strategy=sampling_strategy, version=3)\n",
    "# nm = NearMiss(version=3)\n",
    "X_new, y = nm.fit_resample(X_new, y)\n",
    "\n",
    "# from undersample import HyperSpectralUnderSampler\n",
    "\n",
    "# # ....\n",
    "# undersampler = HyperSpectralUnderSampler(n_clusters=40, samples_per_cluster=15, random_state=123)\n",
    "# x, y = undersampler.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed metric\n",
    "Based on Lasso approach, a metric have been proposed to penalize activation of the features.\n",
    "\n",
    "## Accuracy penalized by number of features\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "    Acc_{penalized} = Acc - \\frac{2 \\alpha Acc}{1 + \\lambda / \\lambda_{max}}\n",
    "\\end{align}$\n",
    "\n",
    "where $Acc$ is the accuracy, $\\lambda_{max}$ the number of features of the original samples, $\\lambda$ the number of features selectioned and $\\alpha$ is a paramter of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_penalized(acc, n_features_selected, n_features, alpha=.5):\n",
    "    penalization = (2*alpha*acc / (1 + (n_features/n_features_selected)))\n",
    "    return acc - penalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 1\n",
    "n_features = 50\n",
    "n_features_selected = np.arange(1,50, dtype=np.uint)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "proposed_alpha = [.2, .25, .35, .5]\n",
    "for alpha_value in proposed_alpha:\n",
    "    acc_penalized = accuracy_penalized(acc, n_features_selected, n_features, alpha=alpha_value)\n",
    "    ax.plot(acc_penalized, label=r\"$\\alpha = {%.2f}$\" % alpha_value)\n",
    "\n",
    "ax.set_xlabel('Features selected')\n",
    "ax.set_ylabel('Penalized Accuracy')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "n_features = X_new.shape[1]\n",
    "\n",
    "search_space = list()\n",
    "search_space.append(Integer(1, 2**(n_features)-1, 'log-uniform', name='transform__selected_features', dtype=np.uint64))\n",
    "search_space.append(Real(1e-6, 100.0, 'log-uniform', name='svc__C'))\n",
    "search_space.append(Categorical(['linear', 'poly', 'rbf', 'sigmoid'], name='svc__kernel'))\n",
    "search_space.append(Integer(1, 5, name='svc__degree'))\n",
    "search_space.append(Real(1e-6, 100.0, 'log-uniform', name='svc__gamma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from HySpecLab.feature_selection.aux import FeatureSelection\n",
    "\n",
    "# define the function used to evaluate a given configuration\n",
    "@use_named_args(search_space) # https://scikit-optimize.github.io/stable/modules/generated/skopt.utils.use_named_args.html\n",
    "def evaluate_model(**params):\n",
    "\t# configure the model with specific hyperparameters\n",
    "\tmodel = Pipeline([(\"transform\", FeatureSelection(n_features=n_features)), ('svc', SVC())])\n",
    "\tmodel.set_params(**params)\n",
    "\t# define test harness\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\t# calculate 5-fold cross validation\n",
    "\tresult = cross_val_score(model, X_new, y.values.flatten(), cv=cv, n_jobs=2, scoring='accuracy')\n",
    "\t# calculate the mean of the scores\n",
    "\testimate = np.mean(result)\n",
    "\t\n",
    "\t# Accuracy penalization based on number of features selected\n",
    "\tif model['transform'].selected_features:\t\t\n",
    "\t\tfeature_idx = model['transform'].getIndex()\n",
    "\t\testimate = accuracy_penalized(estimate, feature_idx.sum(), model['transform'].n_features, alpha=.2)\n",
    "\n",
    "\t# convert from a maximizing score to a minimizing score\n",
    "\treturn 1.0 - estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from skopt import gp_minimize\n",
    "from skopt.callbacks import CheckpointSaver, TimerCallback\n",
    "\n",
    "exp_id = f'{datetime.now().timestamp()}'.split('.')[0]\n",
    "checkpoint_saver = CheckpointSaver(\"./checkpoints/{}.pkl\".format(exp_id), compress=9) # keyword arguments will be passed to `skopt.dump`\n",
    "timer = TimerCallback()\n",
    "\n",
    "result = gp_minimize(evaluate_model, search_space, callback=[checkpoint_saver, timer], random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.iter_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import load\n",
    "res = load('checkpoints/{}.pkl'.format(exp_id)) # Select a checkpint \n",
    "x0 = res.x_iters\n",
    "y0 = res.func_vals\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
