{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.getcwd()\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DermaDataset\n",
    "\n",
    "dataset_root_dir = \"/home/abian/Data/Dataset/IUMA/DermaDatabase/dataCubes/\"\n",
    "train_dir = ['train', 'validation']\n",
    "dataset_dir = list(map(lambda x: os.path.join(dataset_root_dir, x), train_dir))\n",
    "\n",
    "dataset = DermaDataset(dataset_dir)\n",
    "x, y = dataset.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset balancing\n",
    "**https://imbalanced-learn.org/stable/under_sampling.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "# Randomly selecting a subset of data for the targeted classes:\n",
    "# rus = RandomUnderSampler(random_state=123)\n",
    "# x, y = rus.fit_resample(x, y)\n",
    "\n",
    "# Let positive samples be the samples belonging to the targeted class to be under-sampled. \n",
    "# Negative sample refers to the samples from the minority class.\n",
    "# Select the positive samples for which the average distance to the N closest samples of the negative class is the smallest\n",
    "nm1 = NearMiss(version=1)\n",
    "x, y = nm1.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction\n",
    "\n",
    "Due to a computational limitation, it is now only possible to apply the selection of features by Bayesian optimization up to 64 features. This step is used in order to reduce the number of features based on Tree-based feature importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# clf = ExtraTreesClassifier(n_estimators=150)\n",
    "clf = RandomForestClassifier(random_state=123)\n",
    "clf_params={'n_estimators':[50, 150, 500, 1000]}\n",
    "\n",
    "fs_clf = GridSearchCV(clf, clf_params, cv=10,iid=False, n_jobs=-1)\n",
    "fs_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best estimator: {}\".format(fs_clf.best_estimator_))\n",
    "fi = fs_clf.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0075 #Empirical value\n",
    "fs_model = SelectFromModel(fs_clf.best_estimator_, threshold=threshold, prefit=True)\n",
    "X_new = fs_model.transform(x)\n",
    "\n",
    "print(\"Features selected: {}\".format(np.where(fs_clf.best_estimator_.feature_importances_ > threshold)[0]))\n",
    "print(\"X shape: {}\".format(X_new.shape))\n",
    "\n",
    "n_features=X_new.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed metric\n",
    "## Accuracy penalized by number of features\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "    Acc_{penalized} = Acc - \\frac{2 \\alpha Acc}{1 + \\lambda / \\lambda_{max}}\n",
    "\\end{align}$\n",
    "\n",
    "where $Acc$ is the accuracy, $\\lambda_{max}$ the number of features of the original samples, $\\lambda$ the number of features selectioned and $alpha$ is a paramter of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_penalized(acc, n_features_selected, n_features, alpha=.5):\n",
    "    penalization = (2*alpha*acc / (1 + (n_features/n_features_selected)))\n",
    "    return acc - penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection import FeatureSelection, FeatureEquidistantSelection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = list()\n",
    "# search_space.append(Integer(1, float(2**(116)-1), 'log-uniform', name='transform__selected_features'))\n",
    "search_space.append(Integer(1, 2**(n_features)-1, 'log-uniform', name='transform__selected_features', dtype=np.uint64))\n",
    "search_space.append(Real(1e-6, 100.0, 'log-uniform', name='svc__C'))\n",
    "search_space.append(Categorical(['linear', 'poly', 'rbf', 'sigmoid'], name='svc__kernel'))\n",
    "search_space.append(Integer(1, 5, name='svc__degree'))\n",
    "search_space.append(Real(1e-6, 100.0, 'log-uniform', name='svc__gamma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function used to evaluate a given configuration\n",
    "@use_named_args(search_space) # https://scikit-optimize.github.io/stable/modules/generated/skopt.utils.use_named_args.html\n",
    "def evaluate_model(**params):\n",
    "\t# configure the model with specific hyperparameters\n",
    "\tmodel = Pipeline([(\"transform\", FeatureSelection(n_features=n_features)), ('svc', SVC())])\n",
    "\tmodel.set_params(**params)\n",
    "\t# define test harness\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\t# calculate 5-fold cross validation\n",
    "\tresult = cross_val_score(model, X_new, y, cv=cv, n_jobs=-1, scoring='accuracy')\n",
    "\t# calculate the mean of the scores\n",
    "\testimate = np.mean(result)\n",
    "\t\n",
    "\t# Accuracy penalization based on number of features selected\n",
    "\tif model['transform'].selected_features:\t\t\n",
    "\t\tfeature_idx = model['transform'].getIndex()\n",
    "\t\testimate = accuracy_penalized(estimate, feature_idx.sum(), model['transform'].n_features, alpha=.25)\n",
    "\n",
    "\t# convert from a maximizing score to a minimizing score\n",
    "\treturn 1.0 - estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.callbacks import CheckpointSaver\n",
    "from datetime import datetime\n",
    "\n",
    "exp_id = f'{datetime.now().timestamp()}'.split('.')[0]\n",
    "checkpoint_saver = CheckpointSaver(\"./checkpoints/{}.pkl\".format(exp_id), compress=9) # keyword arguments will be passed to `skopt.dump`\n",
    "\n",
    "result = gp_minimize(evaluate_model, search_space, callback=[checkpoint_saver], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing finding:\n",
    "print('Best Accuracy: %.3f' % (1.0 - result.fun))\n",
    "print('Best Parameters: %s' % (result.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue search from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import load\n",
    "res = load('checkpoints/1633957078.pkl') # Select a checkpint \n",
    "x0 = res.x_iters\n",
    "y0 = res.func_vals\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gp_minimize(\n",
    "            evaluate_model, \n",
    "            search_space,\n",
    "            x0=x0,              # already examined values for x\n",
    "            y0=y0,              # observed values for x0\n",
    "            callback=[checkpoint_saver], \n",
    "            random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af680f298c3d17ec705d00ff05262bced2955ca0c4523d2b3ea9cadbd32c0648"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('HySpecLab': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
