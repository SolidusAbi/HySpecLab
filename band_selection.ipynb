{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import os, sys\n",
    "project_dir = os.getcwd()\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "from spectral.algorithms import spectral_angles\n",
    "from dataset import DermaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/home/abian/Data/Dataset/IUMA/DermaDatabase/dataCubes/\"\n",
    "train_dir = ['train', 'validation']\n",
    "dataset_dir = list(map(lambda x: os.path.join(dataset_root_dir, x), train_dir))\n",
    "\n",
    "dataset = DermaDataset(dataset_dir)\n",
    "x, y = dataset.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Calibration \n",
    "\n",
    "$\\begin{align}\n",
    "    C_i = 100 * \\frac{R_i - D_r}{W_r - D_r}\n",
    "\\end{align}$\n",
    "\n",
    "where $C_i$ is the calibrated image, $R_i$ note raw image and the $W_r$ and $D_r$ represents the white and dark reference image, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_r y D_r es una imagen\n",
    "def calibrate(img, w_r, d_r):\n",
    "    if not(w_r.shape == d_r.shape == img.shape):\n",
    "        assert('Dimensionality error')\n",
    "    \n",
    "    return 100  * (img - d_r) / (w_r - d_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "$\\begin{align}\n",
    "    P'_i = \\frac{P_i - P_{min}}{P_{max} - P_{min}}\n",
    "\\end{align}$\n",
    "\n",
    "where $P'_i$ is the normalized pixel value, $P_i$ the reflectance of the pixel, $P_{min}$ and $P_{max}$ is the minimum and maximum reflectance value, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Interval Analysis\n",
    "\n",
    "Selected equidistant band ...\n",
    "\n",
    "$\\begin{align}\n",
    "    Sampling Interval (nm) = \\frac{\\lambda_{max} - \\lambda_{min}}{N_{\\lambda}}\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda_{max} - \\lambda_{min}$ is the difference between the mamum and minimum wavelength and $N_{\\lambda}$ is the number of band captured by the sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_interval(lambda_min, lambda_max, n_spectral_bands):\n",
    "    '''\n",
    "        Param:\n",
    "        -----\n",
    "            lambda_min (int): minimum wavelenght\n",
    "            lambda_max (int): maximum wavelenght\n",
    "            n_spectral_bands (int): number of spectral bands captured by the sensor\n",
    "    '''\n",
    "    return (lambda_max - lambda_min) / n_spectral_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reduction\n",
    "\n",
    "**Spectral Angle Mapper**\n",
    "\n",
    "$\\alpha = cos^{-1}\\left ( \\frac{\\sum_{i = 1}^{nb} t_{i} r_{i}}{(\\sum_{i = 1}^{nb} t_{i}^2)^{\\frac{1}{2}} (\\sum_{i = 1}^{nb} r_{i}^2)^{\\frac{1}{2}}} \\right )$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\alpha$ = spectral angle between the standard and the spectral curve of the pixel\n",
    "* $nb$ = number of spectral channels\n",
    "* $t$ = vector of spectral response of the standard\n",
    "* $r$ = the spectral response vector of the analyzed pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.algorithms import spectral_angles\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def spectral_angles_pixel(x, ref):\n",
    "    '''\n",
    "        For pixel input, the original function is prepare for image\n",
    "    '''\n",
    "    return spectral_angles(x[np.newaxis,:], ref)[0]\n",
    "\n",
    "def get_most_relevant_samples(x, centroid, n_samples_per_centroid=10):\n",
    "    '''\n",
    "        Paper: Most Relevant Spectral Bands Identification for Brain\n",
    "        Cancer Detection Using Hyperspectral Imaging    \n",
    "    '''\n",
    "    if len(x.shape) != 2:\n",
    "        assert 'X shape error!'\n",
    "    \n",
    "    output = None\n",
    "    result = spectral_angles_pixel(x, centroid)\n",
    "    for i in range(len(centroid)):       \n",
    "        ind = np.argpartition(result[i], -n_samples_per_centroid)[-n_samples_per_centroid:]\n",
    "        if i == 0:\n",
    "            output = x[ind]\n",
    "        else:\n",
    "            output = np.concatenate([output, x[ind]], axis=0)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_most_relevant_samples_idx(x, centroid, n_samples_per_centroid=20):\n",
    "    '''\n",
    "        Paper: Most Relevant Spectral Bands Identification for Brain\n",
    "        Cancer Detection Using Hyperspectral Imaging    \n",
    "    '''\n",
    "    if len(x.shape) != 2:\n",
    "        assert 'X shape error!'\n",
    "    \n",
    "    output = np.array([])\n",
    "    result = spectral_angles_pixel(x, centroid)\n",
    "    for i in range(len(centroid)):       \n",
    "        ind = np.argpartition(result[i], -n_samples_per_centroid)[-n_samples_per_centroid:]\n",
    "        output = np.concatenate((output, ind))\n",
    "\n",
    "    return output\n",
    "\n",
    "def dataset_reduction(x, y, n_centroid_per_class=100, random_state=123):\n",
    "    class_label = np.unique(y)\n",
    "    final_x = None\n",
    "    final_y = None\n",
    "    for i in range(len(class_label)):\n",
    "        print('CLass: {}'.format(i))\n",
    "        idx = np.where(y==class_label[i])\n",
    "        print(idx)\n",
    "        _x = x[idx]\n",
    "        test = np.array([])\n",
    "        if len(idx[0]) > 1000:\n",
    "            kmeans = KMeans(n_clusters=n_centroid_per_class, random_state=random_state).fit(_x)\n",
    "            centroid = kmeans.cluster_centers_\n",
    "            # _x = get_most_relevant_samples_idx(_x, centroid)\n",
    "            test = np.concatenate([test, get_most_relevant_samples_idx(_x, centroid)]) \n",
    "            print(test)\n",
    "\n",
    "    #     if i == 0:\n",
    "    #         final_x = _x\n",
    "    #         final_y = np.full((_x.shape[0],), class_label[i])\n",
    "    #     else:\n",
    "    #         final_x = np.concatenate([final_x, _x], axis=0)\n",
    "    #         final_y = np.concatenate([final_y, np.full((_x.shape[0],), class_label[i])], axis=0)\n",
    "    \n",
    "    # return final_x, final_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLass: 0\n",
      "(array([    0,     1,     2, ..., 10587, 10588, 10589]),)\n",
      "(3343, 116)\n",
      "(3343, 100)\n",
      "[19. 92. 97. ...  3. 37. 99.]\n",
      "CLass: 1\n",
      "(array([  100,   101,   102, ..., 10635, 10636, 10637]),)\n",
      "(7295, 116)\n",
      "(7295, 100)\n",
      "[44. 48. 13. ...  3.  1. 49.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12926/3239627712.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_red\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_centroid_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_red\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_red\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "x_red, y_red = dataset_reduction(x, y, n_centroid_per_class=100)\n",
    "print(x_red.shape)\n",
    "print(y_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([])\n",
    "test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling.base import BaseUnderSampler\n",
    "from spectral.algorithms import spectral_angles\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Por ahora no usar!!! Está en pruebas!!\n",
    "\n",
    "class HyperSpectralUnderSampler(BaseUnderSampler):\n",
    "    ''' \n",
    "        Class to perform HyperSpectral data under-sampling.\n",
    "\n",
    "        Under-sample the different class(es) by K-Mean unsupervised clustering approach. The K-Means clustering \n",
    "        is applied independently to each group of labeled pixels in order to obtain K clusters per group. In order \n",
    "        to reduce the original training dataset, such centroids are employed to identify the most representative pixels of\n",
    "        each class by using the Spectral Angle [2] algorithm. For each cluster centroid, only the S most similar\n",
    "        samples are selected.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_clusters: int, default=100\n",
    "            The number of centroids used in K-Mean clustering (K).\n",
    "        \n",
    "        samples_per_class: int, default=10\n",
    "            The number of most similiar signals to select (S)\n",
    "\n",
    "        {random_state}\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "          [1] Martinez, B., Leon, R., Fabelo, H., Ortega, S., Piñeiro, J. F., Szolna, A., ... & M Callico, G. (2019). Most\n",
    "          relevant spectral bands identification for brain cancer detection using hyperspectral imaging.Sensors, 19(24), 5481.\n",
    "          \n",
    "          [2] Rashmi, S.; Addamani, S.; Ravikiran, A. Spectral Angle Mapper algorithm for remote sensing image classification. \n",
    "          IJISET Int. J. Innov. Sci. Eng. Technol. 2014, 1, 201–20\n",
    "    '''\n",
    "    def __init__(self, *, n_clusters=100, samples_per_class=10, random_state=None):\n",
    "        self.K = n_cluster\n",
    "        self.S = samples_per_class\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __spectral_angles_pixel(self, x, ref):\n",
    "        '''\n",
    "            For pixel input, the original function is prepare for image\n",
    "        '''\n",
    "        return spectral_angles(x[np.newaxis,:], ref)[0]\n",
    "\n",
    "    def _get_most_relevant_samples(self, x, centroid, n_samples_per_centroid=10):\n",
    "        if len(x.shape) != 2:\n",
    "            assert 'X shape error!'\n",
    "        \n",
    "        output = None\n",
    "        result = self.__spectral_angles_pixel(x, centroid)\n",
    "        for i in range(len(centroid)):       \n",
    "            ind = np.argpartition(result[i], -n_samples_per_centroid)[-n_samples_per_centroid:]\n",
    "            if i == 0:\n",
    "                output = x[ind]\n",
    "            else:\n",
    "                output = np.concatenate([output, x[ind]], axis=0)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # Mmmmm.... esperate esperate...\n",
    "        class_label = np.unique(y)\n",
    "        final_x = None\n",
    "        final_y = None\n",
    "        for i in range(len(class_label)):\n",
    "            idx = np.where(y==class_label[i])\n",
    "            _x = X[idx]\n",
    "            if len(idx[0]) > 1000:\n",
    "                kmeans = KMeans(n_clusters=self.K, random_state=self.random_state).fit(_x)\n",
    "                centroid = kmeans.cluster_centers_\n",
    "                _x = self._get_most_relevant_samples(_x, centroid, self.S)\n",
    "\n",
    "            if i == 0:\n",
    "                final_x = _x\n",
    "                final_y = np.full((_x.shape[0],), class_label[i])\n",
    "            else:\n",
    "                final_x = np.concatenate([final_x, _x], axis=0)\n",
    "                final_y = np.concatenate([final_y, np.full((_x.shape[0],), class_label[i])], axis=0)\n",
    "        \n",
    "\n",
    "        self.sample_indices_ = 0 # To finish\n",
    "        return final_x, final_y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 116)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "x_red, y_red = dataset_reduction(x, y)\n",
    "print(x_red.shape)\n",
    "print(y_red.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Steps involved in HyperOptimization using Scikit-Optimizer\n",
    "\n",
    "1. Define the space of hyperparameters to search\n",
    "1. Define the function used to evaluate a given configuration\n",
    "1. Minimize the loss using Space and Function defined in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "\n",
    "from feature_selection import FeatureSelection, FeatureEquidistantSelection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer\n",
    "\n",
    "\n",
    "# pipe = Pipeline([(\"transform\", FeatureEquidistantSelection()), ('svc', SVC())])\n",
    "pipe = Pipeline([(\"transform\", FeatureSelection()), ('svc', SVC())])\n",
    "\n",
    "params = dict()\n",
    "n_features = x.shape[1]\n",
    "# params['transform__n_features_to_select'] = (8, 34, 'uniform')\n",
    "params['transform__selected_features'] = Integer(1, float(2**(116)-1), 'log-uniform')\n",
    "params['svc__C'] = (1e-6, 100.0, 'log-uniform')\n",
    "params['svc__gamma'] = (1e-6, 100.0, 'log-uniform')\n",
    "params['svc__degree'] = (1,5)\n",
    "params['svc__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the search\n",
    "search = BayesSearchCV(estimator=pipe, search_spaces=params, n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "search.fit(x, y)\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ant Colony Optimization (Testing)\n",
    "\n",
    "**Buff, no furula. Buscar otra lib?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probar **scikit-opt**: https://github.com/guofei9987/scikit-opt\n",
    "\n",
    "https://www.youtube.com/watch?v=YFN_fJEu63w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for _ in range(20):\n",
    "  x = np.random.uniform(-10, 10)\n",
    "  y = np.random.uniform(-10, 10)\n",
    "  nodes.append((x, y))\n",
    "\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pants\n",
    "import math\n",
    "\n",
    "def euclidean(a, b):\n",
    "    return math.sqrt(pow(a[1] - b[1], 2) + pow(a[0] - b[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pants.World(nodes, euclidean)\n",
    "solver = pants.Solver()\n",
    "# solution = solver.solve(world)\n",
    "\n",
    "solutions = solver.solutions(world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(solution.distance)\n",
    "# print(solution.tour)    # Nodes visited in order\n",
    "# print(len(solution.tour))\n",
    "# print(solution.path)    # Edges taken in order\n",
    "# print(len(solution.path))\n",
    "\n",
    "best = float(\"inf\")\n",
    "for solution in solutions:\n",
    "    assert solution.distance < best\n",
    "    best = solution.distance\n",
    "    print(best)\n",
    "    print(len(solution.path))\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each centroid, only the 10 most similar pixels are selected, having a total of 1000 pixels per class (100 centroids ×10 pixels). Thus, the reduced dataset is\n",
    "intended to avoid the inclusion of redundant information in the training of the supervised classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.trace_elite[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = 2\n",
    "\n",
    "X = np.array([[1, 2.5], [1, 4.1], [1, 0.1],\n",
    "        [10, 2.1], [10, 4.9], [10, 0]])\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X)\n",
    "centroid = kmeans.cluster_centers_\n",
    "for x in X:\n",
    "    plt.scatter(x=x[0], y=x[1], alpha=.2)\n",
    "\n",
    "for x in centroid:\n",
    "    plt.scatter(x=x[0], y=x[1], marker='*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af680f298c3d17ec705d00ff05262bced2955ca0c4523d2b3ea9cadbd32c0648"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('HySpecLab': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
