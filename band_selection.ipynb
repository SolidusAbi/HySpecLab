{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import os, sys\n",
    "project_dir = os.getcwd()\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "from spectral.algorithms import spectral_angles\n",
    "from dataset import DermaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/home/abian/Data/Dataset/IUMA/DermaDatabase/dataCubes/\"\n",
    "train_dir = ['train', 'validation']\n",
    "dataset_dir = list(map(lambda x: os.path.join(dataset_root_dir, x), train_dir))\n",
    "\n",
    "dataset = DermaDataset(dataset_dir)\n",
    "x, y = dataset.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Calibration \n",
    "\n",
    "$\\begin{align}\n",
    "    C_i = 100 * \\frac{R_i - D_r}{W_r - D_r}\n",
    "\\end{align}$\n",
    "\n",
    "where $C_i$ is the calibrated image, $R_i$ note raw image and the $W_r$ and $D_r$ represents the white and dark reference image, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_r y D_r es una imagen\n",
    "def calibrate(img, w_r, d_r):\n",
    "    if not(w_r.shape == d_r.shape == img.shape):\n",
    "        assert('Dimensionality error')\n",
    "    \n",
    "    return 100  * (img - d_r) / (w_r - d_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "$\\begin{align}\n",
    "    P'_i = \\frac{P_i - P_{min}}{P_{max} - P_{min}}\n",
    "\\end{align}$\n",
    "\n",
    "where $P'_i$ is the normalized pixel value, $P_i$ the reflectance of the pixel, $P_{min}$ and $P_{max}$ is the minimum and maximum reflectance value, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Interval Analysis\n",
    "\n",
    "Selected equidistant band ...\n",
    "\n",
    "$\\begin{align}\n",
    "    Sampling Interval (nm) = \\frac{\\lambda_{max} - \\lambda_{min}}{N_{\\lambda}}\n",
    "\\end{align}$\n",
    "\n",
    "where $\\lambda_{max} - \\lambda_{min}$ is the difference between the mamum and minimum wavelength and $N_{\\lambda}$ is the number of band captured by the sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_interval(lambda_min, lambda_max, n_spectral_bands):\n",
    "    '''\n",
    "        Param:\n",
    "        -----\n",
    "            lambda_min (int): minimum wavelenght\n",
    "            lambda_max (int): maximum wavelenght\n",
    "            n_spectral_bands (int): number of spectral bands captured by the sensor\n",
    "    '''\n",
    "    return (lambda_max - lambda_min) / n_spectral_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reduction\n",
    "\n",
    "**Spectral Angle Mapper**\n",
    "\n",
    "$\\alpha = cos^{-1}\\left ( \\frac{\\sum_{i = 1}^{nb} t_{i} r_{i}}{(\\sum_{i = 1}^{nb} t_{i}^2)^{\\frac{1}{2}} (\\sum_{i = 1}^{nb} r_{i}^2)^{\\frac{1}{2}}} \\right )$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\alpha$ = spectral angle between the standard and the spectral curve of the pixel\n",
    "* $nb$ = number of spectral channels\n",
    "* $t$ = vector of spectral response of the standard\n",
    "* $r$ = the spectral response vector of the analyzed pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.algorithms import spectral_angles\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def spectral_angles_pixel(x, ref):\n",
    "    '''\n",
    "        For pixel input, the original function is prepare for image\n",
    "    '''\n",
    "    return spectral_angles(x[np.newaxis,:], ref)[0]\n",
    "\n",
    "def get_most_relevant_samples(x, centroid, n_samples_per_centroid=10):\n",
    "    '''\n",
    "        Paper: Most Relevant Spectral Bands Identification for Brain\n",
    "        Cancer Detection Using Hyperspectral Imaging    \n",
    "    '''\n",
    "    if len(x.shape) != 2:\n",
    "        assert 'X shape error!'\n",
    "    \n",
    "    output = None\n",
    "    result = spectral_angles_pixel(x, centroid)\n",
    "    for i in range(len(centroid)):       \n",
    "        ind = np.argpartition(result[i], -n_samples_per_centroid)[-n_samples_per_centroid:]\n",
    "        if i == 0:\n",
    "            output = x[ind]\n",
    "        else:\n",
    "            output = np.concatenate([output, x[ind]], axis=0)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_most_relevant_samples_idx(x, centroid, n_samples_per_centroid=10):\n",
    "    '''\n",
    "        Paper: Most Relevant Spectral Bands Identification for Brain\n",
    "        Cancer Detection Using Hyperspectral Imaging    \n",
    "    '''\n",
    "    if len(x.shape) != 2:\n",
    "        assert 'X shape error!'\n",
    "    \n",
    "    output = np.array([], dtype=np.uint)\n",
    "    result = spectral_angles_pixel(x, centroid)\n",
    "    print(result.shape)\n",
    "    for i in range(len(centroid)):       \n",
    "        ind = np.argpartition(result[:, i], -n_samples_per_centroid)[-n_samples_per_centroid:].astype(np.uint)\n",
    "        output = np.concatenate((output, ind))\n",
    "\n",
    "    return output\n",
    "\n",
    "def dataset_reduction(x, y, n_centroid_per_class=100, random_state=123):\n",
    "    class_label = np.unique(y)\n",
    "    final_x = None\n",
    "    final_y = None\n",
    "    final_idx = np.array([])\n",
    "    for i in range(len(class_label)):\n",
    "        print('CLass: {}'.format(i))\n",
    "        idx = np.where(y==class_label[i])\n",
    "        _x = x[idx]\n",
    "        # print(_x)\n",
    "        if len(idx[0]) > 1000:\n",
    "            kmeans = KMeans(n_clusters=n_centroid_per_class, random_state=random_state).fit(_x)\n",
    "            centroid = kmeans.cluster_centers_\n",
    "            # _x = get_most_relevant_samples_idx(_x, centroid)\n",
    "            most_relevant_samples_idx = get_most_relevant_samples_idx(_x, centroid)\n",
    "            # print(idx[0][most_relevant_samples_idx])\n",
    "            final_idx = np.concatenate((final_idx, idx[0][most_relevant_samples_idx]))\n",
    "            # test = np.concatenate([test, get_most_relevant_samples_idx(_x, centroid)]) \n",
    "            # print(test)\n",
    "\n",
    "    #     if i == 0:\n",
    "    #         final_x = _x\n",
    "    #         final_y = np.full((_x.shape[0],), class_label[i])\n",
    "    #     else:\n",
    "    #         final_x = np.concatenate([final_x, _x], axis=0)\n",
    "    #         final_y = np.concatenate([final_y, np.full((_x.shape[0],), class_label[i])], axis=0)\n",
    "    \n",
    "    return final_idx\n",
    "    # return final_x, final_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3343,)\n",
      "(7295,)\n"
     ]
    }
   ],
   "source": [
    "class_label = np.unique(y)\n",
    "for i in range(len(class_label)):\n",
    "    class_idx = np.argwhere(y==class_label[i]).flatten()\n",
    "    _x = x[class_idx]\n",
    "    kmeans = KMeans(n_clusters=100, random_state=123).fit(_x)\n",
    "    centroid = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    print(labels.shape)\n",
    "    # result = spectral_angles_pixel(x, centroid)\n",
    "    # output = np.array([], dtype=np.uint)\n",
    "    # for i in range(len(centroid)):       \n",
    "    #     ind = np.argpartition(result[:, i], -10)[-10:].astype(np.uint)\n",
    "    #     output = np.concatenate((output, ind))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  100   101   102 ... 10635 10636 10637]\n"
     ]
    }
   ],
   "source": [
    "idx = np.argwhere(y==class_label[i]).flatten()\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_samples_idx(x, labels, centroids, n_samples_per_centroid=10):\n",
    "    '''\n",
    "        Returns the index of samples where \n",
    "\n",
    "        Paper: Most Relevant Spectral Bands Identification for Brain\n",
    "        Cancer Detection Using Hyperspectral Imaging    \n",
    "\n",
    "        Params:\n",
    "        ------\n",
    "            x (np.ndarray): Data\n",
    "            labels (np.ndarray): Cluster label of x\n",
    "            centroids (np.ndarray): The centroids from clustering algorithm\n",
    "            n_samples_per_centroid: int, 10\n",
    "    '''\n",
    "    if len(x.shape) != 2:\n",
    "        assert 'X shape error!'\n",
    "\n",
    "    if x.shape != labels.shape:\n",
    "        assert 'x and labels do not match in dimension'\n",
    "\n",
    "    if np.unique(labels).size != centroids.shape[0]:\n",
    "        assert 'The number of labels and centroids does not match'\n",
    "    \n",
    "    sam_result = spectral_angles_pixel(x, centroids)\n",
    "    selected_samples_idx = np.array([], dtype=np.uint)\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        cluster_samples_idx = np.argwhere(labels==label).flatten()\n",
    "        n_samples_per_centroid = n_samples_per_centroid if cluster_samples_idx.size >= n_samples_per_centroid else cluster_samples_idx.size\n",
    "        ind = np.argpartition(sam_result[cluster_samples_idx, label], -n_samples_per_centroid)[-n_samples_per_centroid:].astype(np.uint)\n",
    "        selected_samples_idx = np.concatenate((selected_samples_idx, cluster_samples_idx[ind])).astype(np.uint)\n",
    "\n",
    "    return selected_samples_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_centroid=20\n",
    "random_state=123\n",
    "\n",
    "def dataset_reduction(x, y, n_clusters=100, random_state=123):\n",
    "    class_label = np.unique(y)\n",
    "    selected_samples_idx = np.array([], dtype=np.uint)\n",
    "    for i in class_label:\n",
    "        class_samples_idx = np.argwhere(y==i).flatten()\n",
    "        _x = x[class_samples_idx]\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(_x)\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        selected_samples_class_idx = get_most_relevant_samples_idx(_x, labels, centroids, n_samples_per_centroid)\n",
    "        selected_samples_idx = np.concatenate((selected_samples_idx, class_samples_idx[selected_samples_class_idx])).astype(np.uint)\n",
    "\n",
    "    return selected_samples_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(selected_samples_idx.shape)\n",
    "test = y[selected_samples_idx]\n",
    "print(test[test==0].shape)\n",
    "# print(selected_samples_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "(100, 116)\n",
      "[301 261 262 ... 621 529 677]\n",
      "1\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "(100, 116)\n",
      "[2326 2755 2396 ...  867  581  420]\n"
     ]
    }
   ],
   "source": [
    "test = dataset_reduction(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = np.unique(y)\n",
    "for i in class_label:\n",
    "    class_idx = np.argwhere(y==i).flatten()\n",
    "    _x = x[class_idx]\n",
    "    kmeans = KMeans(n_clusters=100, random_state=123).fit(_x)\n",
    "    centroid = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 116)\n"
     ]
    }
   ],
   "source": [
    "print(centroid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_centroid = 10\n",
    "# mapper_result = spectral_angles_pixel(_x, centroid)\n",
    "# selected_samples_idx = np.array([], dtype=np.uint)\n",
    "\n",
    "result = get_most_relevant_samples_idx(_x, labels, centroid, n_samples_per_centroid=n_samples_per_centroid)\n",
    "# print(result)\n",
    "print(np.unique(result).size == result.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_centroid = 10\n",
    "mapper_result = spectral_angles_pixel(_x, centroid)\n",
    "selected_samples_idx = np.array([], dtype=np.uint)\n",
    "\n",
    "for label in np.unique(labels):\n",
    "    samples_idx = np.argwhere(labels==label).flatten()\n",
    "    ind = np.argpartition(mapper_result[samples_idx, i], -n_samples_per_centroid)[-n_samples_per_centroid:].astype(np.uint)\n",
    "    selected_samples_idx = np.concatenate((selected_samples_idx, samples_idx[ind])).astype(np.uint)\n",
    "    # print(selected_samples_idx)\n",
    "    # print(samples_idx[ind])\n",
    "    # if label == 2:\n",
    "    #     break\n",
    "# labels[labels==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(selected_samples_idx.shape)\n",
    "print(np.unique(selected_samples_idx).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x[samples_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(c.shape[0]):\n",
    "    plt.plot(c[i])\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)\n",
    "print(np.unique(output).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "for i in range(100):\n",
    "    min_value = np.min(result[:, i])\n",
    "    a = np.concatenate([a, np.argwhere(result[:, i] == min_value).reshape(-1)])\n",
    "\n",
    "np.unique(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_red, y_red = dataset_reduction(x, y, n_centroid_per_class=100)\n",
    "samples_idx = dataset_reduction(x, y, n_centroid_per_class=100)\n",
    "samples_idx\n",
    "print(np.unique(samples_idx))\n",
    "# print(x_red.shape)\n",
    "# print(y_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = y[samples_idx.astype(np.uint)]\n",
    "len(test[test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([])\n",
    "test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sampling_strategy : float, str, dict, callable, default='auto'\\n        Sampling information to sample the data set.\\n\\n        - When ``float``, it corresponds to the desired ratio of the number of\\n          samples in the minority class over the number of samples in the\\n          majority class after resampling. Therefore, the ratio is expressed as\\n          :math:`\\\\alpha_{us} = N_{m} / N_{rM}` where :math:`N_{m}` is the\\n          number of samples in the minority class and\\n          :math:`N_{rM}` is the number of samples in the majority class\\n          after resampling.\\n\\n          .. warning::\\n             ``float`` is only available for **binary** classification. An\\n             error is raised for multi-class classification.\\n\\n        - When ``str``, specify the class targeted by the resampling. The\\n          number of samples in the different classes will be equalized.\\n          Possible choices are:\\n\\n            ``'majority'``: resample only the majority class;\\n\\n            ``'not minority'``: resample all classes but the minority class;\\n\\n            ``'not majority'``: resample all classes but the majority class;\\n\\n            ``'all'``: resample all classes;\\n\\n            ``'auto'``: equivalent to ``'not minority'``.\\n\\n        - When ``dict``, the keys correspond to the targeted classes. The\\n          values correspond to the desired number of samples for each targeted\\n          class.\\n\\n        - When callable, function taking ``y`` and returns a ``dict``. The keys\\n          correspond to the targeted classes. The values correspond to the\\n          desired number of samples for each class.\""
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseUnderSampler._sampling_strategy_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling.base import BaseUnderSampler\n",
    "from imblearn.utils import Substitution\n",
    "from imblearn.utils._docstring import _n_jobs_docstring\n",
    "from spectral.algorithms import spectral_angles\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import _safe_indexing\n",
    "\n",
    "\n",
    "# @Substitution(\n",
    "#     sampling_strategy=BaseUnderSampler._sampling_strategy_docstring,\n",
    "#     n_jobs=_n_jobs_docstring,\n",
    "# )\n",
    "## Por ahora no usar!!! Está en pruebas!!\n",
    "\n",
    "class HyperSpectralUnderSampler(BaseUnderSampler):\n",
    "    ''' \n",
    "        Class to perform HyperSpectral data under-sampling.\n",
    "\n",
    "        Under-sample the different class(es) by K-Mean unsupervised clustering approach. The K-Means clustering \n",
    "        is applied independently to each group of labeled pixels in order to obtain K clusters per group. In order \n",
    "        to reduce the original training dataset, such centroids are employed to identify the most representative pixels of\n",
    "        each class by using the Spectral Angle [2] algorithm. For each cluster centroid, only the S most similar\n",
    "        samples are selected.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_clusters: int, default=100\n",
    "            The number of centroids used in K-Mean clustering (K).\n",
    "        \n",
    "        samples_per_class: int, default=10\n",
    "            The number of most similiar signals to select (S)\n",
    "\n",
    "        {random_state}\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "          [1] Martinez, B., Leon, R., Fabelo, H., Ortega, S., Piñeiro, J. F., Szolna, A., ... & M Callico, G. (2019). Most\n",
    "          relevant spectral bands identification for brain cancer detection using hyperspectral imaging.Sensors, 19(24), 5481.\n",
    "          \n",
    "          [2] Rashmi, S.; Addamani, S.; Ravikiran, A. Spectral Angle Mapper algorithm for remote sensing image classification. \n",
    "          IJISET Int. J. Innov. Sci. Eng. Technol. 2014, 1, 201–20\n",
    "    '''\n",
    "    def __init__(self, *, sampling_strategy=\"auto\", n_clusters=100, samples_per_cluster=10, random_state=None):\n",
    "        super().__init__(sampling_strategy=sampling_strategy)\n",
    "        self.K = n_clusters\n",
    "        self.S = samples_per_cluster\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _spectral_angles_pixel(self, x, ref):\n",
    "        '''\n",
    "            For pixel input, the original function is prepare for image\n",
    "        '''\n",
    "        return spectral_angles(x[np.newaxis,:], ref)[0]\n",
    "\n",
    "    def get_most_relevant_samples_idx(self, x, labels, centroids, n_samples_per_centroid=10):\n",
    "        if len(x.shape) != 2:\n",
    "            assert 'X shape error!'\n",
    "\n",
    "        if x.shape != labels.shape:\n",
    "            assert 'x and labels do not match in dimension'\n",
    "\n",
    "        if np.unique(labels).size != centroids.shape[0]:\n",
    "            assert 'The number of labels and centroids does not match'\n",
    "        \n",
    "        sam_result = self._spectral_angles_pixel(x, centroids)\n",
    "        selected_samples_idx = np.array([], dtype=np.uint)\n",
    "\n",
    "        for label in np.unique(labels):\n",
    "            cluster_samples_idx = np.argwhere(labels==label).flatten()\n",
    "            n_samples_per_centroid = n_samples_per_centroid if cluster_samples_idx.size >= n_samples_per_centroid else cluster_samples_idx.size\n",
    "            ind = np.argpartition(sam_result[cluster_samples_idx, label], -n_samples_per_centroid)[-n_samples_per_centroid:].astype(np.uint)\n",
    "            selected_samples_idx = np.concatenate((selected_samples_idx, cluster_samples_idx[ind])).astype(np.uint)\n",
    "\n",
    "        return selected_samples_idx\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        class_label = np.unique(y)\n",
    "        selected_samples_idx = np.array([], dtype=np.uint)\n",
    "        for i in class_label:\n",
    "            class_samples_idx = np.argwhere(y==i).flatten()\n",
    "            _x = x[class_samples_idx]\n",
    "            kmeans = KMeans(n_clusters=self.K, random_state=self.random_state).fit(_x)\n",
    "            labels = kmeans.labels_\n",
    "            centroids = kmeans.cluster_centers_\n",
    "\n",
    "            selected_samples_class_idx = self.get_most_relevant_samples_idx(_x, labels, centroids, self.S)\n",
    "            selected_samples_idx = np.concatenate((selected_samples_idx, class_samples_idx[selected_samples_class_idx])).astype(np.uint)\n",
    "\n",
    "        self.sample_indices_ = selected_samples_idx\n",
    "\n",
    "        return _safe_indexing(X, self.sample_indices_), _safe_indexing(y, self.sample_indices_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy = {0: 10, 1: 20}\n",
    "a = HyperSpectralUnderSampler(sampling_strategy=sampling_strategy, n_clusters=50, samples_per_cluster=10, random_state=123)\n",
    "test_x, test_y = a.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (3343,)\n",
      "20 (7295,)\n"
     ]
    }
   ],
   "source": [
    "for target_class in a.sampling_strategy_.keys():\n",
    "    n_samples = a.sampling_strategy_[target_class]\n",
    "    target_class_indices = np.flatnonzero(y == target_class)\n",
    "\n",
    "    print(n_samples, target_class_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "# test_x.shape\n",
    "print(test_y.shape)\n",
    "print(test_y[test_y==0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_red, y_red = dataset_reduction(x, y)\n",
    "print(x_red.shape)\n",
    "print(y_red.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Steps involved in HyperOptimization using Scikit-Optimizer\n",
    "\n",
    "1. Define the space of hyperparameters to search\n",
    "1. Define the function used to evaluate a given configuration\n",
    "1. Minimize the loss using Space and Function defined in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "\n",
    "from feature_selection import FeatureSelection, FeatureEquidistantSelection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer\n",
    "\n",
    "\n",
    "# pipe = Pipeline([(\"transform\", FeatureEquidistantSelection()), ('svc', SVC())])\n",
    "pipe = Pipeline([(\"transform\", FeatureSelection()), ('svc', SVC())])\n",
    "\n",
    "params = dict()\n",
    "n_features = x.shape[1]\n",
    "# params['transform__n_features_to_select'] = (8, 34, 'uniform')\n",
    "params['transform__selected_features'] = Integer(1, float(2**(116)-1), 'log-uniform')\n",
    "params['svc__C'] = (1e-6, 100.0, 'log-uniform')\n",
    "params['svc__gamma'] = (1e-6, 100.0, 'log-uniform')\n",
    "params['svc__degree'] = (1,5)\n",
    "params['svc__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the search\n",
    "search = BayesSearchCV(estimator=pipe, search_spaces=params, n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "search.fit(x, y)\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ant Colony Optimization (Testing)\n",
    "\n",
    "**Buff, no furula. Buscar otra lib?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probar **scikit-opt**: https://github.com/guofei9987/scikit-opt\n",
    "\n",
    "https://www.youtube.com/watch?v=YFN_fJEu63w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for _ in range(20):\n",
    "  x = np.random.uniform(-10, 10)\n",
    "  y = np.random.uniform(-10, 10)\n",
    "  nodes.append((x, y))\n",
    "\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pants\n",
    "import math\n",
    "\n",
    "def euclidean(a, b):\n",
    "    return math.sqrt(pow(a[1] - b[1], 2) + pow(a[0] - b[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pants.World(nodes, euclidean)\n",
    "solver = pants.Solver()\n",
    "# solution = solver.solve(world)\n",
    "\n",
    "solutions = solver.solutions(world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(solution.distance)\n",
    "# print(solution.tour)    # Nodes visited in order\n",
    "# print(len(solution.tour))\n",
    "# print(solution.path)    # Edges taken in order\n",
    "# print(len(solution.path))\n",
    "\n",
    "best = float(\"inf\")\n",
    "for solution in solutions:\n",
    "    assert solution.distance < best\n",
    "    best = solution.distance\n",
    "    print(best)\n",
    "    print(len(solution.path))\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each centroid, only the 10 most similar pixels are selected, having a total of 1000 pixels per class (100 centroids ×10 pixels). Thus, the reduced dataset is\n",
    "intended to avoid the inclusion of redundant information in the training of the supervised classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.trace_elite[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = 2\n",
    "\n",
    "X = np.array([[1, 2.5], [1, 4.1], [1, 0.1],\n",
    "        [10, 2.1], [10, 4.9], [10, 0]])\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X)\n",
    "centroid = kmeans.cluster_centers_\n",
    "for x in X:\n",
    "    plt.scatter(x=x[0], y=x[1], alpha=.2)\n",
    "\n",
    "for x in centroid:\n",
    "    plt.scatter(x=x[0], y=x[1], marker='*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('HySpecLab': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
